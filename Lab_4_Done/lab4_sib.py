# -*- coding: utf-8 -*-
"""lab4_sib.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DOGRqbqVkpy6Ps3W9uiNp_gJMZxJTfyz
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget -O kp_train.csv https://www.dropbox.com/scl/fi/mzvuxvjheh6sjzzu8nio9/kp_train.csv?rlkey=dm2k7kbgwhs2bebj3lejzg2ag&dl=0
# !wget -O kp_valid.csv https://www.dropbox.com/scl/fi/4g75r45ntfxsv2evbpr9u/kp_valid.csv?rlkey=631t34qfnq38rn92akc5h10pi&dl=0

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings(action="ignore")
import tensorflow as tf

df_train = pd.read_csv("kp_train.csv", sep='|')
df_val = pd.read_csv("kp_valid.csv", sep='|')

df_train.head(10)

df_val.head(10)

df_train.isna().sum()

df_train.shape

df_train.info()

df = pd.concat([df_train, df_val])
le = LabelEncoder()
df["sentiment"] = le.fit_transform(df["sentiment"])
df.head()

df.sentiment.unique()

# le = LabelEncoder()
# df_val["sentiment"] = le.fit_transform(df_val["sentiment"])
# df_val.head()

# df_val.sentiment.unique()



def standardize_text(df, content_field):
    df[content_field] = df[content_field].str.replace(r"http\S+", "")
    df[content_field] = df[content_field].str.replace(r"http", "")
    df[content_field] = df[content_field].str.replace(r"@\S+", "")
    df[content_field] = df[content_field].str.replace(r"[^A-Za-z0-9(),!?@\'\`\"\_\n]", " ")
    df[content_field] = df[content_field].str.replace(r"@", "at")
    df[content_field] = df[content_field].str.lower()
    return df

standardize_text(df,"review")

# standardize_text(df_val,"review")

import re
import string
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer

df = df.reset_index(drop=True)

lemmatizer=WordNetLemmatizer()
corpus = []
for i in range(0,len(df)):
    review = re.sub('[^a-zA-Zа-яА-Я0-9]', ' ', str(df['review'][i]))
    review = review.split()
    review=[word for word in review if not word in set(stopwords.words('russian'))]
    review = [lemmatizer.lemmatize(word) for word in review]
    review=' '.join(review)
    corpus.append(review)

df_train_len=len(df_train)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(ngram_range=(1, 3))
X = tfidf.fit_transform(corpus)
y = df["sentiment"]

X_train = X[:df_train_len]
X_valid = X[df_train_len:]
y_train = y[:df_train_len]
y_valid = y[df_train_len:]

from sklearn.svm import LinearSVC
classifier = LinearSVC()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_valid)
y_pred[:5]

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
acc = accuracy_score(y_pred, y_valid)
report = classification_report(y_pred, y_valid)
cm = confusion_matrix(y_pred,y_valid)
sns.heatmap(cm, annot=True)
print(report)
print("Accuracy Score of SVC:", acc*100,"%")