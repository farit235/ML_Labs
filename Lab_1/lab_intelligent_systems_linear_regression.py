# -*- coding: utf-8 -*-
"""lab_intelligent_systems_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hLoseqzAKL53M-k1V5wOJQ5VLnmsOlas

# Лабораторная работа 1
*********
## Линейная регрессия как задача контролируемого (индуктивного) обучения

На этом занятии компьютерного практикума вы изучите линейную регрессию и получите
представление, о том, как данная процедура применяется для обработки данных. Во многих
приложениях нейронные сети реализуют регрессионные вычисления (статистический метод
исследования влияния одной или нескольких независимых переменных
$x_1$, $x_2$, ..., $x_p$ на зависимую переменную $y$) или решение задач классификации. При этом, в частности, линейная
регрессия (англ. Linear regression) представляет собой регрессионную модель зависимости одной
(объясняемой, зависимой) переменной $y$ от другой или нескольких других переменных (факторов,
регрессоров, независимых переменных) $x$ с линейной функцией зависимости.

Прежде чем приступить, собственно к программированию, настоятельно рекомендуется
ознакомиться с материалом лекций, а также с дополнительными материалами, имеющими
отношение к задаче градиентного спуска и к области минимизации функционалов
"""

# Commented out IPython magic to ensure Python compatibility.
# импортируем все необходимые билиотеки
import numpy as np
import pandas as pd
import seaborn as sns
import math
from mpl_toolkits.mplot3d import axes3d
from matplotlib import cm
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""## Лабораторная работа 1.1: Простая линейная регрессия

### Линейна регрессия с одной переменной
В этой части упражнения Вы реализуете линейную регрессию с одной переменной для
прогнозирования прибыли сервисного центра по обслуживанию, например, изделий бытовой
электроники. Допустим, рассматриваются кандидатуры нескольких городов для такого сервисного
центра. Сеть подобных сервисных центров уже функционирует и у Вас есть данные по прибыли в
зависимости от числа жителей города (или от количества изделий, находящихся на обслуживании
в городе). Вы бы хотели использовать эти данные с тем, чтобы понять, в каком городе открыть
дополнительный сервисный центр или спрогнозировать прибыль в зависимости от числа жителей.
Файл Service_center.csv содержит набор данных для задачи с линейной регрессией. Первый столбец
– число жителей города (количество проданных изделий бытовой электроники), второй – прибыль
центра в этом городе. Отрицательные значения прибыли означают убыток.

### Построение данных
До начала выполнения любого задания было бы полезным представить данные в графическом
виде. Для визуализации набора данных (в дальнейшем, при решении аналогичной задачи с
помощью нейронной сети, этот набор данных представляет собой обучающие данные) можно
использовать точечную диаграмму, т.к. имеется только два свойства – прибыль и население города
(количество аппаратов). Многие задачи из реальной жизни имеют большее количество свойств и
параметров, так что построить их на двумерном графике, конечно же, не удастся

## Задание 1: Отображение данных на экране

Перед началом работы у вас есть возможность выбрать формат файла, с которым вы будетет работать. Для работы с файлами формата .csv предлагается использовать библиотеку pandas. Но также вы можете загрузить данные из файла .txt с помощью функции np.loadtxt('.txt', delimiter=',' ), используя билиотеку numpy
"""

df = pd.read_csv('service_center.csv')

df.head()

"""Реализуйте функцию plot_data с помощью библиотеки matplotlib"""

def plotData(X, y):
  '''
  Инструкция: Отобразите на графике исходные обучающие данные, используя
              команды "figure", "plot" или "scatter". Создайте подписи осей графиков,
              применяя команды "xlabel" и "ylabel". Определите, что данные
              о количестве населения и соответствующем доходе от продаж
              передаются в функцию plotData(x, y) в виде ее
              аргументов x and y
  '''
  # Ваш код здесь
  plt.scatter(X, y, marker='o', color='purple')
  plt.xlabel('Population of City')
  plt.ylabel('Profit')
  plt.grid()
  plt.show()

#строим набор наших данных
plotData(df['population'], df['profit'])

"""## Задание 2: Метод градиентного спуска для одной переменной"""

m = len(df) # Число элементов обучающего набора

X = np.stack([np.ones(m), df['population']], axis=1) # Добавляем единичный столбец к Х
y = np.array(df['profit'])
y = y.reshape((m, 1))
print(X[0:5])

theta = np.zeros((2, 1)) # Инициализируем начальные значения
print(theta)

"""Ваша задача рассчитать значение весовой функции для задачи линейной регрессии. Значение весовой функции вычисляется в зависимости от параметра theta, т.е. параметра линейной регресии, обеспечивающего соответствие значений данных (тренировочного набора) X и y.

**Cost function:**
$$
\begin{aligned}
J(θ) &= \frac{1}{2m}\sum_{i=1}^{m}\left(h_{θ}x_i - y_i\right)^2
\end{aligned}
$$

"""

def computeCost(X, y, theta):
  """
  Инструкция: Вычисляйте стоимость J как результат выбора параметра theta.
              В результате стоимость будет определяться величиной J.
  """
  m = len(X)
  J=0
  # TODO: Implement cost function
  h = X.dot(theta)
  J = (1/(2*m)) * np.sum(np.square(h-y))
  return J

"""В результате выполнения функции computeCost Вы должны получить стоимость равную 32.07"""

print(computeCost(X, y, theta))

"""Напомним, что параметры модели, это значения $θ_j$. Именно их необходимо подобрать с целью минимизации стоимости $J(θ)$. Один из способов это сделать - использовать алгоритм наискорейшего спуска, где каждая итерация обновляет парметры модели:
$$
\begin{aligned}
θ_j &= θ_j - α\frac{1}{m}\sum_{i=1}^{m}\left(h_{θ}x_i - y_i\right)x_j^i
\end{aligned}
$$

"""

def gradientDescent(X, y, theta, alpha, num_iters):
  m = len(y) # number of training examples
  J_history = np.zeros((num_iters, 1))

  for i in range(num_iters):
        h = X.dot(theta)
        theta = theta - (alpha/m) * (X.T.dot(h-y))
        J_history[i] = computeCost(X, y, theta)

  #Сохраняйте функцию стоимости на каждой итерации
  #J_history[i] = computeCost(X, y, theta)
  return theta, J_history

iterations = 2000 # Количество итераций
alpha = 0.01 #Скорость обучения
theta, J_history = gradientDescent(X, y, theta, alpha, iterations) # Выполнение градиентного спуска
theta

J_history

plt.figure(figsize=(9,7))
plt.plot(range(iterations), J_history, label="cost function")
plt.title('График изменения значения функции стоимости')
plt.xlabel('Количество итераций')
plt.ylabel('Функция стоимости')
plt.grid(alpha=0.2)
plt.legend()
plt.show()

"""Вы можете проверить насколько правильно ваш алгоритм нашел параметры модели с помщью библиотеки scipy. В коде ниже реализована минимизация функции стоимости с помощью данной библиотеки, методом BFGS"""

import scipy
from scipy.optimize import minimize
def f_cost(x):
    theta_0, theta_1 = x
    return (np.sum((1/(2*len(df['profit'])))*((theta_0 + df['population']*theta_1 - df['profit'])**2)))

x0 = np.array([1, 1])
result = scipy.optimize.minimize(f_cost, x0, method='BFGS')
result

min_theta_0 = result.x[0]
min_theta_1 = result.x[1]
min_J = result.fun
print(min_theta_0, min_theta_1, min_J, sep='\n')

min_theta_0 = theta[0]
min_theta_1 = theta[1]
min_J = J_history.min()
print(*min_theta_0, *min_theta_1, min_J, sep='\n')

plt.figure(figsize=(9,7))
plt.scatter(df['population'], df['profit'], label="training data", s = 150, c='purple', marker='o',  alpha=0.55)
plt.plot(X[:,1], X.dot(theta), label="linear regression")
plt.xlabel('Число жителей города (десятки тысяч)')
plt.ylabel('Прибыль (10 тыс руб)')
plt.grid(alpha=0.2)
plt.legend()
plt.show()

"""Прогнозирование прибыли для количества проданных изделий 35,000 и 70,00
(грубая оценка количества жителей города)
"""

predict1 = np.array([1, 3.5]).dot(theta)
predict2 = np.array([1, 7]).dot(theta)

print('Для количества изделий = 35,000, предсказываем прибыль: {:.2f} \n'.format(*predict1*10000))
print('Для количества изделий = 70,000, предсказываем прибыль: {:.2f} \n'.format(*predict2*10000))

"""## Задание 3: Визуализация $J$($theta_0$, $theta_1$)"""

# Сетка, на которой рассчитывается J
theta0_vals = np.linspace(-10, 10, 100);
theta1_vals = np.linspace(-1, 4, 100);

theta0_ww, theta1_ww = np.meshgrid(theta0_vals, theta1_vals)

# Инициализация J_vals to a matrix of 0's
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

# Заполнение J_vals
for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = [theta0_vals[i], theta1_vals[j]]
        J_vals[i,j] = computeCost(X, y, t)

w0 =  np.linspace(-10, 10, 100)
w1 =  np.linspace(-1, 4, 100)

ww0, ww1 = np.meshgrid(w0, w1)

sse = [] #Sum of Squared Errors
for j in range(len(w1)):
    sse.append([])
    for i in range(len(w0)):
         sse[j].append((1/(2*len(df['profit'])))*((ww0[j][i] + df['population']*ww1[j][i] - df['profit'])**2).sum())
sse = np.array(sse)

"""Если раскомментировать первую строчку следующего блока, то у вас будет возможность повернуть получившуюся поверхность и наглядно увидеть минимум. Однако, если взять и прологорифмировать функцию стоимости, то глобальный минимум функции будет виден ещё лучше. Этот минимум и есть оптимальная точка $θ_0$ и $θ_1$ к которой градиентный спуск с каждым шагом всё ближе и ближе"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib notebook
from mpl_toolkits.mplot3d import axes3d

fig = plt.figure(figsize=(9,7))
ax = fig.add_subplot(111, projection='3d')
ax.view_init(20, 335)

# ax.plot_surface(ww0, ww1, sse, cmap=plt.cm.Spectral, rstride=3, cstride=3)
ax.plot_wireframe(ww0, ww1, sse, color = 'purple', rstride=3, cstride=3, antialiased=True, label = 'function cost' )

# точка - минимум
ax.scatter(min_theta_0, min_theta_1, np.log(min_J),
    color='blue', marker='o', s=300,
    label='min: $θ_0$=%0.2f, $θ_1$=%0.2f, $J$=%0.2f' % (min_theta_0, min_theta_1, min_J))

ax.set_xlabel('$θ_0$', fontsize=12)
ax.set_ylabel('$θ_1$', fontsize=12)
ax.set_zlabel('$J$', fontsize=12)
plt.title('Визуализация функции стоимости', fontsize=15)
plt.yticks(size = 10)
plt.xticks(size = 10)
plt.legend(fontsize=15)
plt.show()
plt.savefig('d.png')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib notebook
from mpl_toolkits.mplot3d import axes3d
from matplotlib import cm

fig = plt.figure(figsize=(9,7))
ax = fig.add_subplot(111, projection='3d')
ax.view_init(5, 335)

# ax.plot_surface(ww0, ww1, np.log(sse), cmap=plt.cm.Spectral, rstride=3, cstride=3)
ax.plot_wireframe(ww0, ww1, np.log(sse), color = 'purple', rstride=3, cstride=3, antialiased=True, label = 'function cost' )

# точка - минимум
ax.scatter(min_theta_0, min_theta_1, np.log(min_J),
    color='blue', marker='o', s=300,
    label='min: $θ_0$=%0.2f, $θ_1$=%0.2f, $log(J)$=%0.2f' % (min_theta_0, min_theta_1, np.log(min_J)))

ax.set_xlabel('$θ_0$', fontsize=12)
ax.set_ylabel('$θ_1$', fontsize=12)
ax.set_zlabel('$log(J)$', fontsize=12)
plt.title('Визуализация функции стоимости', fontsize=15)
plt.yticks(size = 10)
plt.xticks(size = 10)
plt.legend(fontsize=15)
plt.show()
plt.savefig('c.png')

#Контурное представление
plt.figure(figsize=(9,7))
plt.contourf(ww0, ww1, np.log(sse))
# plt.contour(ww0, ww1, np.log(sse))
plt.scatter(min_theta_0, min_theta_1, s = 100, c='red', marker='o',
           label='min: $θ_0$=%0.2f, $θ_1$=%0.2f' % (min_theta_0, min_theta_1))
plt.xlabel('$θ_0$', fontsize=15)
plt.ylabel('$θ_1$', fontsize=15)
plt.yticks(size = 13)
plt.xticks(size = 13)
plt.legend(fontsize=20)
plt.show()
plt.savefig('b.png')

"""# Лабораторная работа 1.2
*********
## Линейная регрессия для нескольких переменных
"""

data = pd.read_csv("cost_apartments.csv")

data.head()

"""## Задание 1: Нормализация признаков и приведение их к нулевому среднему"""

X = np.array(data[['squera','number_rooms']], dtype=np.float)
y = np.array(data['price'], dtype=np.float)
m = y.shape[0]

# Вывод на экран массива данных
print('Первые 10 элементов массива данных:');
print(X[:10])

def featureNormalize(X_norm):
  '''
  Инструкция:   Сначала, для каждого признака из набора данных вычислите
                его среднее значение и произведите его вычитание из набора данных,
                при этом среднее значение сохраните как mu. Далее, рассчитайте
                стандартное (среднеквадратическое) отклонение sigma
                для каждого признака и разделите X-mu на sigma.
                Заметим, что X представляет собой матрицу, в которой каждый
                столбец характеризует признак, а каждая строка - обучающий пример.
                Нормализацию данных следует производить отдельно для каждого
                признака
  '''
  # mu = np.zeros((1, len(X[0])))
  # sigma = np.zeros((1, len(X[0])))
  mu = np.mean(X, axis=0)
  sigma = np.std(X, axis=0)
  X_norm = (X - mu) / sigma
  return X_norm, mu, sigma

X_norm, mu, sigma, = featureNormalize(X)
X = np.hstack((np.ones((m, 1)),X_norm))

X_norm[0:5]

"""## Задание 2: Метод градиентного спуска

computeCostMulti Рассчитывает значение весовой функции для задачи линейной регрессии с несколькими переменными (признаками). J = computeCostMulti(X, y, theta) значение весовой функции вычисляется в зависимости от параметра theta, т.е. параметра линейной регресии, обеспечивающего соответствие значений данных (тренировочного набора)X и y.
"""

def computeCostMulti(X, y, theta):
  m = y.shape[0]
  J = 0
  h = np.dot(X, theta)
  J = (1 / (2 * m)) * np.sum(np.square(h - y))
  return J

"""gradientDescentMulti Определение значения theta методом градиентного спуска theta = gradientDescentMulti(x, y, theta, alpha, num_iters) производит переопределение theta в процессе выполнения num_iters итерационных шагов с параметром скорости обучения alpha"""

def gradientDescentMulti(X, y, theta, alpha, num_iters):

  m = y.shape[0]
  J_history = []

  for i in range(num_iters):
        h = X.dot(theta)
        theta = theta - (alpha/m) * (X.T.dot(h-y))
        J_history.append(computeCostMulti(X, y, theta))

  # Сохраняйте функцию стоимости на каждой итерации
  #J_history.append(computeCostMulti(X, y, theta))

  return theta, J_history

alpha = 0.01
num_iters = 1000
theta = np.zeros(X.shape[1])
# Инициализация theta и нахождение локального экстремума (минимума)
# функции с помощью движения вдоль градиента (градиентный спуск)
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)
print('theta:\n {} \n {} \n {} \n'.format(theta[0], theta[1], theta[2]))
print('J_history первое и последнее значение функции\n {} \n {}'.format(J_history[0], J_history[-1]))

print("%0.5f" % (computeCost(X, y, theta)))

# Вывод на экран графика сходимости процесса
plt.figure(figsize=(9,7))
plt.plot(range(num_iters), J_history, label = "learning rate {:.2f}".format(alpha))
plt.xlabel('Число итераций')
plt.ylabel('Функция стоимости J')
plt.grid(alpha=0.2)
plt.legend()
plt.show()
plt.savefig('a.png')

num_iters_test = 150
theta_test = np.zeros(X_norm.shape[1])
_ , J_history_1 = gradientDescentMulti(X_norm, y, theta_test, 0.01, num_iters_test)
_ , J_history_2 = gradientDescentMulti(X_norm, y, theta_test, 0.1, num_iters_test)
_ , J_history_3 = gradientDescentMulti(X_norm, y, theta_test, 0.03, num_iters_test)
_ , J_history_4 = gradientDescentMulti(X_norm, y, theta_test, 0.3, num_iters_test)

# Вывод на экран графика сходимости процесса
fig = plt.figure(figsize=(9,7))
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('Число итераций')
ax.set_ylabel('Функция стоимости J')
plt.title(' Сходимость метода градиентного спуска с соответствующим диапазоном скоростей обучения\n')
ax.plot(np.arange(len(J_history_1)), J_history_1, lw=2, c='r', label = "learning rate {:.2f}".format(0.01))
ax.plot(np.arange(len(J_history_2)), J_history_2, lw=2, c='g', label = "learning rate {:.2f}".format( 0.1))
ax.plot(np.arange(len(J_history_3)), J_history_3, lw=2, c='b', label = "learning rate {:.2f}".format(0.03))
ax.plot(np.arange(len(J_history_4)), J_history_4, lw=2, c='y', label = "learning rate {:.2f}".format( 0.3))
plt.grid(alpha=0.2)
plt.legend()
plt.show()
plt.savefig('res.png')

"""Практическое задание: оценка стоимости трехкомнатной квартиры площадью 60 м2"""

in_x = np.array([1, 60, 3])
norm_mu = np.concatenate([[0], mu], axis=0)
norm_sigma = np.concatenate([[1], sigma], axis=0)
norm_in_x = ((in_x - norm_mu)/norm_sigma).T
price = np.dot((theta.T), norm_in_x);

print('Стоимость трехкомнатной квартиры площадью 60 м2 оцененная методом')
print(f'градиентного спуска составляет: {price} рублей\n')

"""## Задание 3: Система нормальных уравнений"""

X = np.array(data[['squera','number_rooms']], dtype=np.float)
y = np.array(data['price'], dtype=np.float)
m = y.shape[0]
X = np.concatenate([np.ones((m, 1)), X], axis=1)
print(X[0:5])

"""Замкнутая форма для линейной регрессии записывается как:: $$ w = (X^TX)^{-1}X^Ty $$
Использование этой формулы не требует нормализации свойств. Точный результат достигается без использования итераций: нет цикла как в градиентном спуске. Завершите код в normalEqn используя приведённую формулу, для вычисления $θ$. Здесь не требуется нормализация, однако это не значит, что не нужно добавить единичный столбец (учёт $θ_0$) к $X$
"""

from numpy import linalg
def normalEqn(X, y):
    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
    return theta

"""Расчет параметров theta решением системы нормальных уравнений"""

theta_sisteam = normalEqn(X, y)
print("Значение theta, полученное с помощью системы нормальных уравнений: ")
print(theta_sisteam, sep='\n')

"""Используйте этот метод для оценки стоимости трехкомнатной квартиры площадью 60 м2, методом наименьших квадратов. Можете использовать предыдущее задание с использованием метода градиентного спуска для проверки (должно получиться сходное число)."""

in_x_new = np.array([1, 60, 3])
price_sistem = in_x_new.dot(theta_sisteam)
print('Стоимость трехкомнатной квартиры площадью 60 м2, оцененная методом наименьших квадратов составляет: {} рублей'.format(price_sistem));