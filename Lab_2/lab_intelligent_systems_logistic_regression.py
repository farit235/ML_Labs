# -*- coding: utf-8 -*-
"""lab_intelligent_systems_logistic_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iA_dLX5rTTR4DpBKN88PfwjiWxcKalLo

# Лабораторная работа 2
*********
## Предсказание вероятности возникновения события по значениям множества признаков (логистическая регрессия)

На этом занятии компьютерного практикума вы изучите логистическую регрессию и примените метод логистической регрессии к обработке двух различных наборов данных, а именно, данных, характеризующих систему сигнализации об исправности двигателя автомобиля в процессе эксплуатации, и данных, описывающих автомат по отбраковке микрочипов на производстве. Прежде чем приступить, собственно, к программированию, настоятельно рекомендуется ознакомиться с материалом лекций, а также с дополнительными материалами, имеющими отношение к задаче логистической регрессии и классификации.
"""

# Commented out IPython magic to ensure Python compatibility.
# импортируем все необходимые билиотеки
import numpy as np
import pandas as pd
import seaborn as sns
import math
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

import seaborn as sns
sns.set(style='ticks')

"""## Инициализация

Перед началом работы у вас есть возможность выбрать формат файла, с которым вы будетет работать. Для работы с файлами формата .csv предлагается использовать библиотеку pandas. Но также вы можете загрузить данные из файла .txt с помощью функции np.loadtxt('.txt', delimiter=',' ), используя билиотеку numpy
"""

df = pd.read_csv('engine.csv')

df.head()

df.describe()

# Формируем вектор признаков
m = len(df)
x = np.array(df['noise'])
temp = np.array(df['rotation'])
y = np.array(df['class'])

x = x.reshape((m, 1))
temp = temp.reshape((m, 1))
y = y.reshape((m, 1))
X = np.hstack((temp,x))
print(X[0:5])

# Добавляем к вектору признаков столбец единиц для theta_0
t = np.ones((m,1))
X = np.hstack((t,X))
print(X[0:5])

"""## Задание 1. Отображение

До начала выполнения заданий желательно представить данные в графическом виде. В первой части ноутбука, как и в Лабораторной работе 1, посвященной линейной регрессии, для этого будет вызвана функция построения двухмерных графиков plotData. Вам необходимо завершить программу plotData так, чтобы её результатом служил график, подобный приведенному на Рис. 1, где оси соответствуют двум оценкам, а положительные или отрицательные результаты – маркерам разных цветов.
"""

def plotData(X, y, size, marker, color, label):
  '''
  Инструкция: Отобразите на графике исходные обучающие данные, используя
              команды "figure", "scatter". Создайте подписи осей графиков,
              применяя команды "xlabel" и "ylabel".
  '''
  # Ваш код здесь
  plt.scatter(X, y, s=size, c=color, marker=marker, label=label)
  plt.xlabel("Подпись оси X")
  plt.ylabel("Подпись оси Y")
  plt.legend()

#строим набор наших данных
plt.figure(figsize=(13,10))
plotData (df['noise'][df['class'] == 0], df['rotation'][df['class'] == 0], 250, '>', 'green', 'Исправен' )
plotData (df['noise'][df['class'] == 1], df['rotation'][df['class'] == 1], 250, '<', 'red', 'Поломка')
plt.show()

"""## Задание 2: Вычисление функции стоимости и градиентов
#### В этой части задания, определяются функция стоимости и градиенты для задачи логистической регрессии.
"""

[m, n] = X.shape

initial_theta = np.zeros((n , 1))

"""До того, как приступать к написанию функции стоимости, определим гипотезу логистической
регрессии:
$$
\begin{aligned}
h_θ(x) &= g(θ^Tx)
\end{aligned}
$$
где сигмоидная функция g, определяется как:
$$
\begin{aligned}
g(z) &= \frac{1}{1+e^-z}\
\end{aligned}
$$

Сначала следует описать эту функцию в функции sigmoid. Для проверки попробуйте вычислить несколько значений сигмоидной функции, написав sigmoid(x). Для больших положительных значений x, значение сигмоидной функции должно быть близко к 1, а для высоких отрицательных – к 0. Значение функции sigmoid в нуле должно быть равно 0,5. Программа должна работать также с векторами и матрицами. Для работы с матрицами сигмоидная функция должна обрабатывать каждый элемент матрицы по отдельности.
"""

def sigmoid(z):
  """
  Указание:    z может быть матрицей вектором или скаляром.
  """
  # Ваш код здесь
  z = np.array(z)
  g = 1 / (1 + np.exp(-z))
  return g

def costFunction(theta, X, y, return_grad=False):
  """
  Указание: Градиент должен иметь ту же размерность, что и theta
  """
  # Ваш код здесь
  m = len(y)
  #В процессе выполнения задания, следующие переменные должны быть вычислены
  # правильно
  h = sigmoid(np.dot(X, theta))
  J = -1/m * (np.dot(y.T, np.log(h)) + np.dot(1-y.T, np.log(1-h)))
  if return_grad:
      grad = 1/m * np.dot(X.T, (h - y))
      return J, grad

  return J

cost, grad = costFunction(initial_theta, X, y, True)

print('Значение функции стоимости при начальных (нулевых) значениях вектора thetа:', cost, '\n');
print('Значение градиента при начальных (нулевых) значениях вектора thetа:',grad, ' \n');

"""## Задание 3: Оптимизация

В этом задании предполагается вместо уже известного метода градиентного спуска использовать для вычисления оптимальных значений параметров theta библиотеку scipy
"""

from scipy.optimize import fmin
myargs=(X, y)
theta = fmin(costFunction, x0=initial_theta, args=myargs)

costFunction(theta, X, y)

print('Значение theta:',theta)

"""### Отображение границы классов

"""

def plotDecisionBoundary(X, theta, df):
  """
  Указание: Ваша задача разобраться, как строиться граница классов
  """
  x_values = [np.min(X[:, 1]), np.max(X[:, 2])]
  y_values = - (theta[0] + np.dot(theta[1], x_values)) / theta[2]

  plt.figure(figsize=(13,10))
  plt.plot(x_values, y_values, linewidth = 3, color = 'black', label='Граница классов')
  plotData(df['noise'][df['class'] == 0], df['rotation'][df['class'] == 0], 250, '>', 'green', 'Исправен' )
  plotData(df['noise'][df['class'] == 1], df['rotation'][df['class'] == 1], 250, '<', 'red', 'Поломка')

  plt.show()

plotDecisionBoundary(X, theta, df)

"""## Задание 4: Предсказание и оценка точности

После выполнения обучения следует обеспечить предсказание для тех
  данных, которые не были использованы в процессе обучения, т.е. данных,
  которые возникают в процессе эксплуатации автомобиля.
  В этом задании, логистическая регрессия используется для предсказания
  вероятности того, что двигатель, состояние которого характеризуется
  следующими параметрами, шум - 45 ед., вибрация - 85 ед, является
  исправным.

  Более того, Вам предстоит вычислить точность (погрешность)
  используемой логистической модели.

  Задача состоит в завершении кода predict.m

  Определение вероятности работоспособности или отказа двигателя,
  состояние которого характеризуется следующими параметрами, шум - 45 ед.,
  вибрация - 85 ед.
"""

prob = sigmoid(np.array([1, 45, 85]).dot(theta[:, np.newaxis]))

print('Для двигателя с уровнем шума 45 и вибрацией 85, предсказывается поломка с вероятностью:  {:.2%} \n'.format(prob[0]))

def predict(x, y, threshold=0.5):
  """
  PREDICT Отнесение образца к классам 0 или 1 ("исправен" или "не исправен")
  в процессе линейной регрессии на основании оценки theta
  PREDICT обеспечивает классификацию X с пороговым
  значением 0.5 (т.е., если значение сигмоидной функции
  sigmoid(theta'*x) >= 0.5, то присвоение 1)
  """
  # Ваш код здесь
  accuracy = []

  def hFunc(x):
    z = np.matmul(theta.transpose(), x)
    g = sigmoid(z)
    if g >= threshold:
      return 1
    return 0

  count = len(y)
  correct_count = 0

  for i in range(len(X)):
    res = hFunc(X[i])
    if res == y[i]:
      correct_count += 1

  accuracy = correct_count / count
  return accuracy

print('Точность обучения: {:.0%} \n'.format(predict(X, y)/100))

"""## Задание 5: Регуляризованная логистическая регрессия

Исследуемые данные не являются линейно сепарабельными и, следовательно, не могут быть разделены на положительный и отрицательный классы прямой линией. Поэтому буквальное применение метода простой  логистической регрессии не подходит в данном примере, поскольку он подразумевает прямолинейную границу раздела двух областей. Тем не менее, использование логистической регрессии возможно, а именно, посредством применения полиноминальных признаков, подобно полиноминальной регрессии.

## Инициализация
"""

data = pd.read_csv('test.csv')

data.head()

data.describe()

m = len(data)
x = np.array(data['test_1'])
temp = np.array(data['test_2'])
y = np.array(data['class'])

x = x.reshape((m, 1))
temp = temp.reshape((m, 1))
y = y.reshape((m, 1))
X = np.hstack((temp,x))
print(X[0:5])

plt.figure(figsize=(13,10))
plotData(data['test_1'][data['class'] == 0], data['test_2'][data['class'] == 0], 200, 'o', 'blue', 'Class 1' )
plotData(data['test_1'][data['class'] == 1], data['test_2'][data['class'] == 1], 200, 's', 'orange', 'Class 2')
plt.xlabel('Test 1')
plt.ylabel('Test 2')
plt.show()

def mapFeature(X1, X2):
    degree = 6
    out = np.ones(( X1.shape[0], sum(range(degree + 2)) ))
    curr_column = 1
    for i in range(1, degree + 1):
        for j in range(i+1):
            out[:,curr_column] = np.power(X1,i-j) * np.power(X2,j)
            curr_column += 1
    return out

X = mapFeature(X[:,0], X[:,1])

X.shape

initial_theta = np.zeros((X.shape[1], 1))
initial_theta

"""Вычисление и отображение начальных значений функции стоимости и
градиента для регуляризованной логистической регрессии
"""

def costFunctionReg(theta, X, Y, lambda_reg):
  """
  Вычисление функции стоимости и значения градиента(ов)для
  задачи логистической регрессии с регуляризацией
  costFunctionReg(theta, X, y, lambda_reg) вычисляет функцию стоимости, используя
  theta в качестве параметра логистической регрессии, а также значение(я)
  градиентов
  """
  # Ваш код здесь
  cols = X.shape[1]
  def thetaFunc(x, y, z):
    h = hFunc(x)
    left = h - y
    return left * z

  def hFunc(x):
    z = np.matmul(theta.transpose(), x)
    g = sigmoid(z)
    return g

  def jFunc(x, y):
    h = hFunc(x)
    left = -1 * y * np.log(h)
    right = (1 - y) * np.log(1 - h)
    return left - right

  def thetaSum():
    sum = 0
    for i in range(1, len(theta)):
      sum += np.multiply(theta[i], theta[i])
      sum *= lambda_reg / m
    return sum
  m = len(Y)
  J = (sum(map(jFunc, X, Y))) / m
  J += thetaSum()

  return J

cost = costFunctionReg(initial_theta, X, y, 1)

print('Значение функции стоимости при начальном значении theta (zeros):', cost)

def gradFunctionReg(theta, X, y, lambda_reg):
    grad = np.zeros(theta.shape)
    grad = (1./m) * np.dot(sigmoid( np.dot(X,theta) ).T - y, X).T + ( float(lambda_reg) / m )*theta

    return  grad.flatten()

gradFunctionReg(initial_theta, X, y, 1)

"""## Задание 6: Регуляризация и точность

Дополнительное задание:
В этом задании, варьируя значения lambda, выяснить влияние регуляризации на поведение
границы разделения классов

Указание: Продемонстрировать эффект для lambda (0, 1, 10, 100).
"""

def plotDecisionBoundary(theta, lambda_reg):
    plt.figure(figsize=(10,10))

    plotData(data['test_1'][data['class'] == 0], data['test_2'][data['class'] == 0], 200, 'o', 'blue', 'Class 1' )
    plotData(data['test_1'][data['class'] == 1], data['test_2'][data['class'] == 1], 200, 's', 'orange', 'Class 2')


    u = np.linspace(-1, 1.5, 50)
    v = np.linspace(-1, 1.5, 50)
    uu, vv = np.meshgrid(u, v)
    z = np.zeros(( len(u), len(v) ))


    for i in range(len(u)):
        for j in range(len(v)):
            z[i,j] = np.dot(mapFeature(np.array([u[i]]), np.array([v[j]])),theta)
    z = np.transpose(z)

    plt.contour(u, v, z, levels=[0], colors='black', linewidths=3)
    plt.title('\n lambda = {:d}, Точность обучения: {:.2%} \n'.format(lambda_reg, predict(X, y)/100), fontsize=20)

    plt.xlabel('Test 1', fontsize=20)
    plt.ylabel('Test 2', fontsize=20)
    plt.yticks(size = 15)
    plt.xticks(size = 15)
    plt.yticks(size = 15)
    plt.xticks(size = 15)
    plt.legend(fontsize=15)
    plt.show()

from scipy.optimize import fmin_bfgs
lambda_reg = [0, 1, 10, 100, 150]

for lambda_i in lambda_reg:
    myargs=(X, y, lambda_i)
    theta = fmin_bfgs(costFunctionReg, x0 = initial_theta, args = myargs)
    plotDecisionBoundary(theta, lambda_i)

lambda_reg = 0
myargs=(X, y, lambda_reg)
theta = fmin_bfgs(costFunctionReg, x0 = initial_theta, args = myargs)

print('Максимальная точность обучения: {:.2%} \n'.format(predict(X, y)/100))